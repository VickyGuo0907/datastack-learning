{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Modern Data Stack Knowledge \u00b6 This document would collect what I learn and practice related with modern data stack","title":"Home"},{"location":"#modern-data-stack-knowledge","text":"This document would collect what I learn and practice related with modern data stack","title":"Modern Data Stack Knowledge"},{"location":"concept/base-concept/","text":"Basic Concept Knowledge \u00b6 Below would be all related knowledge you should know related with Modern Data Stack ETL vs ELT \u00b6 ETL \u2014 Extract, Transforming, Loading \u00b6 Reference articles: What\u2019s ETL ETL makes it possible to migrate data between a variety of sources, destinations, and analysis tools. As a result, the ETL process plays a critical role in producing business intelligence and executing broader data management strategies. ELT \u2014 Extract, Loading, Transforming \u00b6 Reference articles: What\u2019s ELT Extract/load/transform (ELT) similarly extracts data from one or multiple remote sources, but then loads it into the target data warehouse without any other formatting. The transformation of data, in an ELT process, happens within the target database. ELT asks less of remote sources, requiring only their raw and unprepared data. When is ELT the right choice? \u00b6 When ingestion speed is the number one priority When more intel is better intel When you know you will need to scale ETL vs ELT: Understanding the different \u00b6 The primary differences between ETL and ELT are how much data is retained in data warehouses and where data is transformed. With ETL, the transformation of data is done before it is loaded into a data warehouse. This enables analysts and business users to get the data they need faster, without building complex transformations or persistent tables in their business intelligence tools. Using the ELT approach, data is loaded into the warehouse or data lake as is, with no transformation before loading. This makes jobs easier to configure because it only requires an origin and a destination. The ETL and ELT approaches to data integration differ in several key ways. Load time \u2014 It takes significantly longer to get data from source systems to the target system with ETL. Transformation time \u2014 ELT performs data transformation on-demand, using the target system\u2019s computing power, reducing wait times for transformation. Complexity \u2014 ETL tools typically have an easy-to-use GUI that simplifies the process. ELT requires in-depth knowledge of BI tools, masses of raw data, and a database that can transform it effectively. Data warehouse support \u2014 ETL is a better fit for legacy on-premise data warehouses and structured data. ELT is designed for the scalability of the cloud. Maintenance \u2014 ETL requires significant maintenance for updating data in the data warehouse. With ELT, data is always available in near real-time. ELT and Data Lakes: The Future of Data Integration? \u00b6 Modern, cloud-based infrastructure technologies offer large amounts of data storage and scalable computing power at lower costs, making it possible to keep petabytes of data in large and expandable data lakes, and process it quickly on-demand. The proliferation of data lakes has made it possible for more organizations to move from ETL to ELT. Benefits of using ELT in the cloud Scalability (Almost) seamless integration Open Source Lower cost of ownership OLTP and OLAP \u00b6 Reference Articles: OLTP and OLAP: a practical comparison OLTP \u2014 Online Transaction Processing \u00b6 OLAP \u2014 Online Aanytical Processing \u00b6 Data Lake vs Data WareHouse \u00b6 Data Lake \u00b6 Reference articles: What\u2019s a Data Lake? A data lake is a central storage repository that holds big data from many sources in a raw, granular format.","title":"Base Concept"},{"location":"concept/base-concept/#basic-concept-knowledge","text":"Below would be all related knowledge you should know related with Modern Data Stack","title":"Basic Concept Knowledge"},{"location":"concept/base-concept/#etl-vs-elt","text":"","title":"ETL vs ELT"},{"location":"concept/base-concept/#etl-extract-transforming-loading","text":"Reference articles: What\u2019s ETL ETL makes it possible to migrate data between a variety of sources, destinations, and analysis tools. As a result, the ETL process plays a critical role in producing business intelligence and executing broader data management strategies.","title":"ETL --- Extract, Transforming, Loading"},{"location":"concept/base-concept/#elt-extract-loading-transforming","text":"Reference articles: What\u2019s ELT Extract/load/transform (ELT) similarly extracts data from one or multiple remote sources, but then loads it into the target data warehouse without any other formatting. The transformation of data, in an ELT process, happens within the target database. ELT asks less of remote sources, requiring only their raw and unprepared data.","title":"ELT --- Extract, Loading, Transforming"},{"location":"concept/base-concept/#when-is-elt-the-right-choice","text":"When ingestion speed is the number one priority When more intel is better intel When you know you will need to scale","title":"When is ELT the right choice?"},{"location":"concept/base-concept/#etl-vs-elt-understanding-the-different","text":"The primary differences between ETL and ELT are how much data is retained in data warehouses and where data is transformed. With ETL, the transformation of data is done before it is loaded into a data warehouse. This enables analysts and business users to get the data they need faster, without building complex transformations or persistent tables in their business intelligence tools. Using the ELT approach, data is loaded into the warehouse or data lake as is, with no transformation before loading. This makes jobs easier to configure because it only requires an origin and a destination. The ETL and ELT approaches to data integration differ in several key ways. Load time \u2014 It takes significantly longer to get data from source systems to the target system with ETL. Transformation time \u2014 ELT performs data transformation on-demand, using the target system\u2019s computing power, reducing wait times for transformation. Complexity \u2014 ETL tools typically have an easy-to-use GUI that simplifies the process. ELT requires in-depth knowledge of BI tools, masses of raw data, and a database that can transform it effectively. Data warehouse support \u2014 ETL is a better fit for legacy on-premise data warehouses and structured data. ELT is designed for the scalability of the cloud. Maintenance \u2014 ETL requires significant maintenance for updating data in the data warehouse. With ELT, data is always available in near real-time.","title":"ETL vs ELT: Understanding the different"},{"location":"concept/base-concept/#elt-and-data-lakes-the-future-of-data-integration","text":"Modern, cloud-based infrastructure technologies offer large amounts of data storage and scalable computing power at lower costs, making it possible to keep petabytes of data in large and expandable data lakes, and process it quickly on-demand. The proliferation of data lakes has made it possible for more organizations to move from ETL to ELT. Benefits of using ELT in the cloud Scalability (Almost) seamless integration Open Source Lower cost of ownership","title":"ELT and Data Lakes: The Future of Data Integration?"},{"location":"concept/base-concept/#oltp-and-olap","text":"Reference Articles: OLTP and OLAP: a practical comparison","title":"OLTP and OLAP"},{"location":"concept/base-concept/#oltp-online-transaction-processing","text":"","title":"OLTP --- Online Transaction Processing"},{"location":"concept/base-concept/#olap-online-aanytical-processing","text":"","title":"OLAP --- Online Aanytical Processing"},{"location":"concept/base-concept/#data-lake-vs-data-warehouse","text":"","title":"Data Lake vs Data WareHouse"},{"location":"concept/base-concept/#data-lake","text":"Reference articles: What\u2019s a Data Lake? A data lake is a central storage repository that holds big data from many sources in a raw, granular format.","title":"Data Lake"},{"location":"concept/modern-analytics-stack/","text":"Modern Analytics Stack \u00b6 Reference: Modern Analytics Stack Data Value Chain \u00b6 data process: Specifcation : defining what to track Instrumentation: registering events in your code Collection: ingesting & processing events Integration: adding data from various sources Data Warehousing: storing, processing and serving data Transformation: preparing data for end users. Quality Assurance: bad data = bad decisions Data discovery: finding the right data asset for the problem. Analysis: creating the narrative Observability: monitoring data assets, pipelines & infrastructure","title":"Modern Analytics Stack"},{"location":"concept/modern-analytics-stack/#modern-analytics-stack","text":"Reference: Modern Analytics Stack","title":"Modern Analytics Stack"},{"location":"concept/modern-analytics-stack/#data-value-chain","text":"data process: Specifcation : defining what to track Instrumentation: registering events in your code Collection: ingesting & processing events Integration: adding data from various sources Data Warehousing: storing, processing and serving data Transformation: preparing data for end users. Quality Assurance: bad data = bad decisions Data discovery: finding the right data asset for the problem. Analysis: creating the narrative Observability: monitoring data assets, pipelines & infrastructure","title":"Data Value Chain"},{"location":"concept/modern-data-stack-v2/","text":"Modern Data Stack V2 \u00b6 In this current time of growth in the modern data stack, I believe there are five key areas ripe for innovation: Artificial Intelligence Data Sharing Data Governance Streaming Application Serving Artificial Intelligence \u00b6 If a company doesn\u2019t have a solid story for how they are collecting, storing, and modifying data, any data science project is doomed before it starts because the foundations upon which it rests is rapidly shifting. Reference Articles Emerging Architectures for Modern Data Infrastructure Data-first systems prioritize automation and operationalization of AI, drastically reducing the time it takes to build and maintain new use cases, and push them into production. Data Sharing (Data-as-a-Service) \u00b6 provide an invaluable service to customers that allow users to easily and quickly move data out of their cloud data warehouse and into downstream applications that need it. For a lot of use cases, this is essential, and, without purpose-built tooling, companies would be stuck writing custom integration scripts or bespoke tooling that are difficult to maintain and implement. Existing Services: Census Hightouch Snowflake\u2019s Data Marketplace Databricks Delta Sharing Data Governance \u00b6 Without good governance tools, the modern data stack will likely be too chaotic and unwieldy for large enterprises and their behemoth data volumes. Governance imposes order upon an organization\u2019s data and often breaks down natural barriers that can make discovery and collaboration actually possible! Metadata management tool Streaming (Real Time) \u00b6 Someone who can offer a solution where I don\u2019t need to think about infrastructure, move my data into a new platform, and can simply and reliably get live data via a standard SQL query will be greatly celebrated. Databricks supports structured streaming KsqlDB \u2013 Kafka-native Snowflake data pipeline Application Serving \u00b6 eventually your data platform is so awesome that people will realize that it contains all the data necessary for the killer application they\u2019ve been writing, and they\u2019ll want to hook up to it. This is generally where our ride comes to a grinding halt. Cloud data warehouses are squarely in the category of OLAP, whereas live applications want something with high concurrency/low latency, which is in the OLTP category. This does not compute.","title":"Modern Data Stack V2"},{"location":"concept/modern-data-stack-v2/#modern-data-stack-v2","text":"In this current time of growth in the modern data stack, I believe there are five key areas ripe for innovation: Artificial Intelligence Data Sharing Data Governance Streaming Application Serving","title":"Modern Data Stack V2"},{"location":"concept/modern-data-stack-v2/#artificial-intelligence","text":"If a company doesn\u2019t have a solid story for how they are collecting, storing, and modifying data, any data science project is doomed before it starts because the foundations upon which it rests is rapidly shifting. Reference Articles Emerging Architectures for Modern Data Infrastructure Data-first systems prioritize automation and operationalization of AI, drastically reducing the time it takes to build and maintain new use cases, and push them into production.","title":"Artificial Intelligence"},{"location":"concept/modern-data-stack-v2/#data-sharing-data-as-a-service","text":"provide an invaluable service to customers that allow users to easily and quickly move data out of their cloud data warehouse and into downstream applications that need it. For a lot of use cases, this is essential, and, without purpose-built tooling, companies would be stuck writing custom integration scripts or bespoke tooling that are difficult to maintain and implement. Existing Services: Census Hightouch Snowflake\u2019s Data Marketplace Databricks Delta Sharing","title":"Data Sharing (Data-as-a-Service)"},{"location":"concept/modern-data-stack-v2/#data-governance","text":"Without good governance tools, the modern data stack will likely be too chaotic and unwieldy for large enterprises and their behemoth data volumes. Governance imposes order upon an organization\u2019s data and often breaks down natural barriers that can make discovery and collaboration actually possible! Metadata management tool","title":"Data Governance"},{"location":"concept/modern-data-stack-v2/#streaming-real-time","text":"Someone who can offer a solution where I don\u2019t need to think about infrastructure, move my data into a new platform, and can simply and reliably get live data via a standard SQL query will be greatly celebrated. Databricks supports structured streaming KsqlDB \u2013 Kafka-native Snowflake data pipeline","title":"Streaming (Real Time)"},{"location":"concept/modern-data-stack-v2/#application-serving","text":"eventually your data platform is so awesome that people will realize that it contains all the data necessary for the killer application they\u2019ve been writing, and they\u2019ll want to hook up to it. This is generally where our ride comes to a grinding halt. Cloud data warehouses are squarely in the category of OLAP, whereas live applications want something with high concurrency/low latency, which is in the OLTP category. This does not compute.","title":"Application Serving"},{"location":"concept/modern-data-stack/","text":"Modern Data Stack \u00b6 What is the Modern Data Stack? \u00b6 Reference Articles: The future of modern data stack The Modern Data Stack commonly refers to a collection of technologies that comprise a cloud-native data platform, generally leveraged to reduce the complexity in running a traditional data platform. The individual components are not fixed, but they typically include: \u00b6 A Cloud Data Warehouse, such as Snowflake , Redshift , BigQuery , or Databricks Delta Lake A Data Integration Service, such as Fivetran , Segment , or Airbyte A ELT data transformation tool, almost certainly dbt A BI layer, such as Looker or Mode A Reverse ETL tool, such as Census or Hightouch the following as key capabilities of technology in the modern data stack: \u00b6 Offered as a Managed Service : Requires no or minimal setup and configuration from users and absolutely no engineering required. Users can get started today, and it\u2019s not a vapid marketing promise. Centered around a Cloud Data Warehouse(CDW) : Everything \u201cjust works\u201d off-the-shelf if companies use a popular CDW. By being opinionated about where your data is, you eliminate messy integrations and tools play well together. Democratizes data via a SQL-Centric Ecosystem :Tools are built for data/analytics engineers and business users. These users often know the most about a company\u2019s data, so it makes sense to try to upskill them by giving them tools that speak their language. Elastic Workloads : Pay for what you use. Scale up instantly to handle large workloads. Money is the only scale limitation in the modern cloud. Focus on Operational Workflows(Automation) : Point-and-click tools are nice for low-tech users, but it\u2019s all kind of meaningless if there\u2019s not a viable path to production. Modern data stack tools are often built with automation as a core competency. Popular components in Modern Data Stack \u00b6 Reference Articles: The Modern data stack an overview THe Modern Data Stack: Open Source Edition ETL (Extrac, Transform, Load) Tools \u00b6 Segment Stitch FiveTran AirByte (Open Source) Apache Airflow (Open Source) Data Warehouses, Lakes & Lakehouses: \u00b6 Amazon RedShift Google BigQuery Snowflak Panoply Delta Lake on Databricks Apache Hive Graph Databases & Analysis \u00b6 Neo4j Amazon Neptune Customer Data Platforms \u00b6 Segment Personas mparticle RudderStack (Open Source) Data Transformation Tools \u00b6 Data Build Tools (DBT) Business Intelligence (BI) Tools \u00b6 Looker Mode Analytics Amplitude Data Catalog & Event Discovery, Documentation, & Governance Tools ( metadata management) \u00b6 Castor Atlan Datafold Avo App DataHub ( Open Source) Amundsen ( Open Source) Superset (Open Source) Marquez (Open Source) OpenLineage (Open Framework) Data Pipeline tools \u00b6 argo (Open Source)","title":"Modern Data Stack"},{"location":"concept/modern-data-stack/#modern-data-stack","text":"","title":"Modern Data Stack"},{"location":"concept/modern-data-stack/#what-is-the-modern-data-stack","text":"Reference Articles: The future of modern data stack The Modern Data Stack commonly refers to a collection of technologies that comprise a cloud-native data platform, generally leveraged to reduce the complexity in running a traditional data platform.","title":"What is the Modern Data Stack?"},{"location":"concept/modern-data-stack/#the-individual-components-are-not-fixed-but-they-typically-include","text":"A Cloud Data Warehouse, such as Snowflake , Redshift , BigQuery , or Databricks Delta Lake A Data Integration Service, such as Fivetran , Segment , or Airbyte A ELT data transformation tool, almost certainly dbt A BI layer, such as Looker or Mode A Reverse ETL tool, such as Census or Hightouch","title":"The individual components are not fixed, but they typically include:"},{"location":"concept/modern-data-stack/#the-following-as-key-capabilities-of-technology-in-the-modern-data-stack","text":"Offered as a Managed Service : Requires no or minimal setup and configuration from users and absolutely no engineering required. Users can get started today, and it\u2019s not a vapid marketing promise. Centered around a Cloud Data Warehouse(CDW) : Everything \u201cjust works\u201d off-the-shelf if companies use a popular CDW. By being opinionated about where your data is, you eliminate messy integrations and tools play well together. Democratizes data via a SQL-Centric Ecosystem :Tools are built for data/analytics engineers and business users. These users often know the most about a company\u2019s data, so it makes sense to try to upskill them by giving them tools that speak their language. Elastic Workloads : Pay for what you use. Scale up instantly to handle large workloads. Money is the only scale limitation in the modern cloud. Focus on Operational Workflows(Automation) : Point-and-click tools are nice for low-tech users, but it\u2019s all kind of meaningless if there\u2019s not a viable path to production. Modern data stack tools are often built with automation as a core competency.","title":"the following as key capabilities of technology in the modern data stack:"},{"location":"concept/modern-data-stack/#popular-components-in-modern-data-stack","text":"Reference Articles: The Modern data stack an overview THe Modern Data Stack: Open Source Edition","title":"Popular components in Modern Data Stack"},{"location":"concept/modern-data-stack/#etl-extrac-transform-load-tools","text":"Segment Stitch FiveTran AirByte (Open Source) Apache Airflow (Open Source)","title":"ETL (Extrac, Transform, Load) Tools"},{"location":"concept/modern-data-stack/#data-warehouses-lakes-lakehouses","text":"Amazon RedShift Google BigQuery Snowflak Panoply Delta Lake on Databricks Apache Hive","title":"Data Warehouses, Lakes &amp; Lakehouses:"},{"location":"concept/modern-data-stack/#graph-databases-analysis","text":"Neo4j Amazon Neptune","title":"Graph Databases &amp; Analysis"},{"location":"concept/modern-data-stack/#customer-data-platforms","text":"Segment Personas mparticle RudderStack (Open Source)","title":"Customer Data Platforms"},{"location":"concept/modern-data-stack/#data-transformation-tools","text":"Data Build Tools (DBT)","title":"Data Transformation Tools"},{"location":"concept/modern-data-stack/#business-intelligence-bi-tools","text":"Looker Mode Analytics Amplitude","title":"Business Intelligence (BI) Tools"},{"location":"concept/modern-data-stack/#data-catalog-event-discovery-documentation-governance-tools-metadata-management","text":"Castor Atlan Datafold Avo App DataHub ( Open Source) Amundsen ( Open Source) Superset (Open Source) Marquez (Open Source) OpenLineage (Open Framework)","title":"Data Catalog &amp; Event Discovery, Documentation, &amp; Governance Tools ( metadata management)"},{"location":"concept/modern-data-stack/#data-pipeline-tools","text":"argo (Open Source)","title":"Data Pipeline tools"},{"location":"dataflow/base-concept/","text":"Basic Concept Knowledge \u00b6","title":"Base Concept"},{"location":"dataflow/base-concept/#basic-concept-knowledge","text":"","title":"Basic Concept Knowledge"},{"location":"dataflow/task-tools/","text":"Task Orchestration Tools and Workflow \u00b6 Recently there\u2019s been an explosion of new tools for orchestrating task- and data workflows (sometimes referred to as \u201cMLOps\u201d). The quantity of these tools can make it hard to choose which ones to use and to understand how they overlap, so we decided to compare some of the most popular ones head to head. Overall Apache Airflow is both the most popular tool and also the one with the broadest range of features, but Luigi is a similar tool that\u2019s simpler to get started with. Argo is the one teams often turn to when they\u2019re already using Kubernetes, and Kubeflow and MLFlow serve more niche requirements related to deploying machine learning models and tracking experiments. What is Task orchestration and why is it useful? \u00b6 Workflow orchestration tools allow you to define DAGs \u2013 Directed Acyclic Graph by specifying all of your tasks and how they depend on each other. The tool then executes these tasks on schedule, in the correct order, retrying any that fail before running the next ones. It also monitors the progress and notifies your team when failures happen. Overall, the focus of any orchestration tool is ensuring centralized , repeatable , reproducible , and efficient workflows: a virtual command center for all of your automated tasks. Which to use \u2013 Comparison \u00b6 You should probably use: Apache Airflow if you want the most full-featured, mature tool and you can dedicate time to learning how it works, setting it up, and maintaining it. Luigi if you need something with an easier learning curve than Airflow. It has fewer features, but it\u2019s easier to get off the ground. Prefect if you want something that\u2019s very familiar to Python programmers and stays out of your way as much as possible. Argo if you\u2019re already deeply invested in the Kubernetes ecosystem and want to manage all of your tasks as pods, defining them in YAML instead of Python. KubeFlow if you want to use Kubernetes but still define your tasks with Python instead of YAML. You can also read about our experiences using Kubeflow and why we decided to drop it for our projects at Kubeflow: Not ready for production. MLFlow if you care more about tracking experiments or tracking and deploying models using MLFlow\u2019s predefined patterns than about finding a tool that can adapt to your existing custom workflows. For a quick overview, we\u2019ve compared the libraries when it comes to: Maturity : based on the age of the project and the number of fixes and commits; Popularity : based on adoption and GitHub stars; Simplicity : based on ease of onboarding and adoption; Breadth : based on how specialized vs. how adaptable each project is; Language : based on the primary way you interact with the tool. These are not rigorous or scientific benchmarks, but they\u2019re intended to give you a quick overview of how the tools overlap and how they differ from each other. Reference Article: Workflow Comparision Kubeflow: Not ready for production?","title":"Task Orchestration Tools"},{"location":"dataflow/task-tools/#task-orchestration-tools-and-workflow","text":"Recently there\u2019s been an explosion of new tools for orchestrating task- and data workflows (sometimes referred to as \u201cMLOps\u201d). The quantity of these tools can make it hard to choose which ones to use and to understand how they overlap, so we decided to compare some of the most popular ones head to head. Overall Apache Airflow is both the most popular tool and also the one with the broadest range of features, but Luigi is a similar tool that\u2019s simpler to get started with. Argo is the one teams often turn to when they\u2019re already using Kubernetes, and Kubeflow and MLFlow serve more niche requirements related to deploying machine learning models and tracking experiments.","title":"Task Orchestration Tools and Workflow"},{"location":"dataflow/task-tools/#what-is-task-orchestration-and-why-is-it-useful","text":"Workflow orchestration tools allow you to define DAGs \u2013 Directed Acyclic Graph by specifying all of your tasks and how they depend on each other. The tool then executes these tasks on schedule, in the correct order, retrying any that fail before running the next ones. It also monitors the progress and notifies your team when failures happen. Overall, the focus of any orchestration tool is ensuring centralized , repeatable , reproducible , and efficient workflows: a virtual command center for all of your automated tasks.","title":"What is Task orchestration and why is it useful?"},{"location":"dataflow/task-tools/#which-to-use-comparison","text":"You should probably use: Apache Airflow if you want the most full-featured, mature tool and you can dedicate time to learning how it works, setting it up, and maintaining it. Luigi if you need something with an easier learning curve than Airflow. It has fewer features, but it\u2019s easier to get off the ground. Prefect if you want something that\u2019s very familiar to Python programmers and stays out of your way as much as possible. Argo if you\u2019re already deeply invested in the Kubernetes ecosystem and want to manage all of your tasks as pods, defining them in YAML instead of Python. KubeFlow if you want to use Kubernetes but still define your tasks with Python instead of YAML. You can also read about our experiences using Kubeflow and why we decided to drop it for our projects at Kubeflow: Not ready for production. MLFlow if you care more about tracking experiments or tracking and deploying models using MLFlow\u2019s predefined patterns than about finding a tool that can adapt to your existing custom workflows. For a quick overview, we\u2019ve compared the libraries when it comes to: Maturity : based on the age of the project and the number of fixes and commits; Popularity : based on adoption and GitHub stars; Simplicity : based on ease of onboarding and adoption; Breadth : based on how specialized vs. how adaptable each project is; Language : based on the primary way you interact with the tool. These are not rigorous or scientific benchmarks, but they\u2019re intended to give you a quick overview of how the tools overlap and how they differ from each other. Reference Article: Workflow Comparision Kubeflow: Not ready for production?","title":"Which to use -- Comparison"},{"location":"datagov/metadata-tools/","text":"Metadata Management Tools \u00b6 Datahub \u00b6 Introduction \u00b6 DataHub is an open-source metadata platform for the modern data stack. Read about the architectures of different metadata systems and why DataHub excels here. Also read our LinkedIn Engineering blog post, check out our Strata presentation and watch our Crunch Conference Talk. You should also visit DataHub Architecture to get a better understanding of how DataHub is implemented and DataHub Onboarding Guide to understand how to extend DataHub for your own use cases. DataHub Site Amundsen \u00b6","title":"Metadata Management"},{"location":"datagov/metadata-tools/#metadata-management-tools","text":"","title":"Metadata Management Tools"},{"location":"datagov/metadata-tools/#datahub","text":"","title":"Datahub"},{"location":"datagov/metadata-tools/#introduction","text":"DataHub is an open-source metadata platform for the modern data stack. Read about the architectures of different metadata systems and why DataHub excels here. Also read our LinkedIn Engineering blog post, check out our Strata presentation and watch our Crunch Conference Talk. You should also visit DataHub Architecture to get a better understanding of how DataHub is implemented and DataHub Onboarding Guide to understand how to extend DataHub for your own use cases. DataHub Site","title":"Introduction"},{"location":"datagov/metadata-tools/#amundsen","text":"","title":"Amundsen"}]}